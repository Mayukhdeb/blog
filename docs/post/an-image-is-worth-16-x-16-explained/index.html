<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>An image is worth 16x16 words is worth a lot more words | @mayukhdeb</title>

<meta name="keywords" content="deep-learning" />
<meta name="description" content="Figuring out how an image might be worth 16x16 words">
<meta name="author" content="">
<link rel="canonical" href="https://mayukhdeb.github.io/blog/post/an-image-is-worth-16-x-16-explained/" />
<link href="https://mayukhdeb.github.io/blog/assets/css/stylesheet.min.d1bc2b736056bd5698d770eeedc08a73bce9e6cebb30810f6f1b2c2048e46ab8.css" integrity="sha256-0bwrc2BWvVaY13Du7cCKc7zp5s67MIEPbxssIEjkarg=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://mayukhdeb.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mayukhdeb.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mayukhdeb.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mayukhdeb.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://mayukhdeb.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.68.3" />


<meta property="og:title" content="An image is worth 16x16 words is worth a lot more words" />
<meta property="og:description" content="Figuring out how an image might be worth 16x16 words" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mayukhdeb.github.io/blog/post/an-image-is-worth-16-x-16-explained/" />
<meta property="article:published_time" content="2021-08-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-08-10T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An image is worth 16x16 words is worth a lot more words"/>
<meta name="twitter:description" content="Figuring out how an image might be worth 16x16 words"/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "An image is worth 16x16 words is worth a lot more words",
  "name": "An image is worth 16x16 words is worth a lot more words",
  "description": "Figuring out how an image might be worth 16x16 words",
  "keywords": [
    "deep-learning"
  ],
  "articleBody": "What does the paper try to prove ? The use of transformers for vision has remained quite limited so far. We did see some weird mixtures of convolutional layers and attention, but it turns out that a pure transformer can also perform very well on image classification tasks.\nThe problem with conv layers Conv layers are really good at detecting textures in images, but they’re not at all good at detecting large visual features (this is because of the fact that conv filters are just these tiny sliding windows).\nEach conv filter has a limited receptive field, which grows as we go deeper into the layers. But transformers are able to attend to “everywhere on the image” within a single layer.\nHow do we “tokenize” images ? “Let’s not do local attention over pixels, but global attention over image patches!\"\nThis is how they did it:\n Chop down the image into 16x16 patches Flatten the patches Produce lower dimensional linear embeddings from flattened patches. “Add in/concat” positional embeddings.  And now we have our sequence of “tokens” which we can feed into the model :)\nHold up, what are positional embeddings ?\nIn simple words, they’re “vectors” which tells the transformer where every patch was in an image. These positional embeddings are learnable.\n  Given above is a a brief visual summary of how each image patch is tokenized.\nWhat information does each “token” hold ?\n  “Enter” the transformer Now that we have our “tokens”, the next step is to feed them into the transformer. Let’s first take a look at the original diagram which shows the insides of the transformer:\n  In order to better understand how each of those small boxes work, you should read this other post I wrote.\nAt this point I’ll assume that you already have a basic understanding of stuff like Layer normalization, multi head attention, residual skip connections.\nThe encoder architecture used here is the same as the one we know from the original paper.\nThere is no decoder, Just an extra linear layer for the final classification called MLP head.\n",
  "wordCount" : "345",
  "inLanguage": "en",
  "datePublished": "2021-08-10T00:00:00Z",
  "dateModified": "2021-08-10T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mayukhdeb.github.io/blog/post/an-image-is-worth-16-x-16-explained/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "@mayukhdeb",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mayukhdeb.github.io/blog/favicon.ico"
    }
  }
}
</script>



</head>

<body class="">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mayukhdeb.github.io/blog/" accesskey="h" title="@mayukhdeb (Alt + H)">@mayukhdeb</a>
            <span class="logo-switches">
                <span class="theme-toggle" title="(Alt + T)">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()"></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      An image is worth 16x16 words is worth a lot more words
    </h1>
    <div class="post-meta">August 10, 2021

    </div>
  </header> 

  <div class="post-content">
<h2 id="what-does-the-paper-try-to-prove-">What does the paper try to prove ?<a hidden class="anchor" aria-hidden="true" href="#what-does-the-paper-try-to-prove-">#</a></h2>
<p>The use of transformers for vision has remained quite limited so far.  We did see some weird mixtures of convolutional layers and attention, but it turns out that a pure transformer can also perform very well on image classification tasks.</p>
<h2 id="the-problem-with-conv-layers">The problem with conv layers<a hidden class="anchor" aria-hidden="true" href="#the-problem-with-conv-layers">#</a></h2>
<p>Conv layers are <em>really</em> good at detecting textures in images, but they&rsquo;re not at all good at detecting large visual features (this is because of the fact that conv filters are just these tiny sliding windows).</p>
<p>Each conv filter has a limited receptive field, which grows as we go deeper into the layers. But transformers are able to attend to &ldquo;everywhere on the image&rdquo; within a single layer.</p>
<h3 id="how-do-we-tokenize-images-">How do we &ldquo;tokenize&rdquo; images ?<a hidden class="anchor" aria-hidden="true" href="#how-do-we-tokenize-images-">#</a></h3>
<p><em>&ldquo;Let&rsquo;s not do local attention over pixels, but global attention over image patches!&quot;</em></p>
<p>This is how they did it:</p>
<ol>
<li>Chop down the image into 16x16 patches</li>
<li>Flatten the patches</li>
<li>Produce lower dimensional linear embeddings from flattened patches.</li>
<li>&ldquo;Add in/concat&rdquo; positional embeddings.</li>
</ol>
<p>And now we have our sequence of &ldquo;tokens&rdquo; which we can feed into the model :)</p>
<p><strong>Hold up, what are positional embeddings</strong> ?</p>
<p>In simple words, they&rsquo;re &ldquo;vectors&rdquo; which tells the transformer where every patch was in an image. These positional embeddings are learnable.</p>
<figure>
    <img src="https://github.com/Mayukhdeb/blog/blob/master/content/post/images/2021_aug_10/tokenization.png?raw=true" width="100%"/> 
</figure>

<p>Given above is a a brief visual summary of how each image patch is tokenized.</p>
<p><strong>What information does each &ldquo;token&rdquo; hold</strong> ?</p>
<figure>
    <img src="https://github.com/Mayukhdeb/blog/blob/master/content/post/images/2021_aug_10/each_token.png?raw=true" width="50%"/> 
</figure>

<h3 id="enter-the-transformer">&ldquo;Enter&rdquo; the transformer<a hidden class="anchor" aria-hidden="true" href="#enter-the-transformer">#</a></h3>
<p>Now that we have our &ldquo;tokens&rdquo;, the next step is to feed them into the transformer. Let&rsquo;s first take a look at the original diagram which shows the insides of the transformer:</p>
<figure>
    <img src="https://github.com/Mayukhdeb/blog/blob/master/content/post/images/2021_aug_10/transformer_block.png?raw=true" width="20%"/> 
</figure>

<p>In order to better understand how each of those small boxes work, you should read <a href="https://mayukhdeb.github.io/blog/post/transformers-toolbox/">this other post</a> I wrote.</p>
<p>At this point I&rsquo;ll assume that you already have a basic understanding of stuff like Layer normalization, multi head attention, residual skip connections.</p>
<p>The encoder architecture used here is the same as the one we know from the original paper.</p>
<p>There is no decoder, Just an extra linear layer for the final classification called MLP head.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mayukhdeb.github.io/blog/tags/deep-learning/">Deep learning</a></li>
    </ul>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://mayukhdeb.github.io/blog/">@mayukhdeb</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<button class="top-link" id="top-link" type="button" aria-label="go to top" title="Go to Top (Alt + G)" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <path d="M12 6H0l6-6z" /></svg>
</button>



<script defer src="https://mayukhdeb.github.io/blog/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    mybutton.onclick = function () {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
        window.location.hash = ''
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
