<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Transformer toolbox | @mayukhdeb</title>

<meta name="keywords" content="deep-learning" />
<meta name="description" content="stuff you need to understand before understanding how transformers work">
<meta name="author" content="">
<link rel="canonical" href="https://mayukhdeb.github.io/blog/post/transformers-toolbox/" />
<link href="https://mayukhdeb.github.io/blog/assets/css/stylesheet.min.d1bc2b736056bd5698d770eeedc08a73bce9e6cebb30810f6f1b2c2048e46ab8.css" integrity="sha256-0bwrc2BWvVaY13Du7cCKc7zp5s67MIEPbxssIEjkarg=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://mayukhdeb.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mayukhdeb.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mayukhdeb.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mayukhdeb.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://mayukhdeb.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.68.3" />


<meta property="og:title" content="Transformer toolbox" />
<meta property="og:description" content="stuff you need to understand before understanding how transformers work" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mayukhdeb.github.io/blog/post/transformers-toolbox/" />
<meta property="article:published_time" content="2021-08-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-08-10T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformer toolbox"/>
<meta name="twitter:description" content="stuff you need to understand before understanding how transformers work"/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer toolbox",
  "name": "Transformer toolbox",
  "description": "stuff you need to understand before understanding how transformers work",
  "keywords": [
    "deep-learning"
  ],
  "articleBody": "Before you move on to trying to understand how a transformer works, it’ll be a good warmup to understand some of the key mechanisms that are present within them.\nThis is the stuff you need to know about:\n Tokenization Positional Encodings Vector similarity in N-D spaces Self attention Short residual skip connections Layer Normalization Multi head attention  Tokenization Tokenization is a way of separating a piece of information into smaller units called tokens (of course theyre called tokens).\nTokenization can take different forms depending on the task:\n In NLP, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.  For example, when we apply “word tokenization” to\"hello how are you\", it becomes:\n[ 'hello', 'how', 'are', 'you' ] Here the order of the words is not relevant.\n For vision based tasks, tokenization takes a very different form. One of the ways is to simply chunk the image into smaller patches, flatten them into vectors, concat/add positional embeddings and voilá. The Vision Transformer (ViT) does exactly this - with an additional transformation after the flattening to reduce the dimension of the tokens.  This is how its done:\n Chop down the image into 16x16 patches Flatten the patches Produce lower dimensional linear embeddings from flattened patches. “Add in/concat” positional embeddings.  This section in another post elaborates on how tokenization is acheieved using image patches.\nPositional embeddings When preparing a token, it is always good to also include information on “where it is” and not just “what it is”. This is what a positional embedding is. Generally its concatenated/added into the image/text embedding before feeding it into the transformer encdoer.\nVector similarity in N-D spaces One way to find the similarity between 2 vectors is to determine their scaled dot product, namely the cosine of the angle. It is a stable way of detecting how “equal” two vectors are in relation to both direction and distance. Its values vary from -1 to 1.\nimport math def similarity(v1,v2): \"\"\" This is just a dummy example, please dont use it in practice. compute cosine similarity of v1 to v2: (v1 dot v2)/(||v1||*||v2||) Also see: https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity \"\"\" sumxx, sumxy, sumyy = 0, 0, 0 for i in range(len(v1)): x = v1[i]; y = v2[i] sumxx += x*x sumyy += y*y sumxy += x*y return sumxy/math.sqrt(sumxx*sumyy) x = [-41,-62,-600] y = [2,3,4] print(similarity(x,y)) Self attention In a nutshell, it is a way to differentiably “select/focus on” certain parts of the embedding more than the others. I’ve written a post where I’ve tried to explain how it works from scratch.\nShort residual skip connections You might know this already but they basically act as “information bridges” which helps the gradients flow back from deep into the model into the earlier layers very easily. Stuff that makes up the “Res” in “ResNets”.\nLayer Normalization Let’s first try to see whats batch normalization, this will help us understand layer normalization later on.\nBatch normalization is not needed here, but you should know how it works\nMathematically, BN layer transforms each input in the current mini-batch by:\n Subtracting the input mean in the current mini-batch Dividing it by the standard deviation.  But each layer doesn’t need to expect inputs with zero mean and unit variance, but instead, probably the model might perform better with some other mean and variance. Hence the BN layer also introduces two learnable parameters γ and β (gamma and beta).\nThe important thing to note here is that this normalization works along the batch dim.\nok so how is layer normalization different from batch normalization ?\nThe only difference is that layer normalization individually (along the feature axes) normalizes each feature to have zero mean and unit variance.\nMulti-head attention Multi-head attention is just multiple attention blocks in one single layer whose outputs are concatenated later after the forward passes. The main advantage is that it allows us to attend to different parts of the sequence differently on each “head”.\n In caveman words: multi head attention is just a “bunch” of attention mechanisms in a single forward pass\n   Further reading:   The vision transformer: https://theaisummer.com/vision-transformer/\n  An image is worth 16x16 words, explained: https://www.youtube.com/watch?v=TrdevFK_am4\n  ",
  "wordCount" : "700",
  "inLanguage": "en",
  "datePublished": "2021-08-10T00:00:00Z",
  "dateModified": "2021-08-10T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mayukhdeb.github.io/blog/post/transformers-toolbox/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "@mayukhdeb",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mayukhdeb.github.io/blog/favicon.ico"
    }
  }
}
</script>



</head>

<body class="">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mayukhdeb.github.io/blog/" accesskey="h" title="@mayukhdeb (Alt + H)">@mayukhdeb</a>
            <span class="logo-switches">
                <span class="theme-toggle" title="(Alt + T)">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()"></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      Transformer toolbox
    </h1>
    <div class="post-meta">August 10, 2021

    </div>
  </header> 

  <div class="post-content">
<p>Before you move on to trying to understand how a transformer works, it&rsquo;ll be a good warmup to understand some of the key mechanisms that are present within them.</p>
<p>This is the stuff you need to know about:</p>
<ul>
<li>Tokenization</li>
<li>Positional Encodings</li>
<li>Vector similarity in N-D spaces</li>
<li>Self attention</li>
<li>Short residual skip connections</li>
<li>Layer Normalization</li>
<li>Multi head attention</li>
</ul>
<h2 id="tokenization">Tokenization<a hidden class="anchor" aria-hidden="true" href="#tokenization">#</a></h2>
<p>Tokenization is a way of separating a piece of information into smaller units called tokens (of course theyre called tokens).</p>
<p>Tokenization can take different forms depending on the task:</p>
<ul>
<li><strong>In NLP</strong>, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.</li>
</ul>
<p>For example, when we apply &ldquo;word tokenization&rdquo; to<code>&quot;hello how are you&quot;</code>, it becomes:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">[
    <span style="color:#e6db74">&#39;hello&#39;</span>,
    <span style="color:#e6db74">&#39;how&#39;</span>,
    <span style="color:#e6db74">&#39;are&#39;</span>,
    <span style="color:#e6db74">&#39;you&#39;</span>
]
</code></pre></div><p>Here the order of the words is not relevant.</p>
<ul>
<li><strong>For vision</strong> based tasks, tokenization takes a very different form. One of the ways is to simply chunk the image into smaller patches, flatten them into vectors, concat/add positional embeddings and voilá. The Vision Transformer (ViT) does exactly this - with an additional transformation after the flattening to reduce the dimension of the tokens.</li>
</ul>
<p>This is how its done:</p>
<ol>
<li>Chop down the image into 16x16 patches</li>
<li>Flatten the patches</li>
<li>Produce lower dimensional linear embeddings from flattened patches.</li>
<li>&ldquo;Add in/concat&rdquo; positional embeddings.</li>
</ol>
<p><a href="https://mayukhdeb.github.io/blog/post/an-image-is-worth-16-x-16-explained/#how-do-we-tokenize-images">This section</a> in another post elaborates on how tokenization is acheieved using image patches.</p>
<h2 id="positional-embeddings">Positional embeddings<a hidden class="anchor" aria-hidden="true" href="#positional-embeddings">#</a></h2>
<p>When preparing a token, it is always good to also include information on &ldquo;where it is&rdquo; and not just &ldquo;what it is&rdquo;. This is what a positional embedding is. Generally its concatenated/added into the image/text embedding before feeding it into the transformer encdoer.</p>
<h2 id="vector-similarity-in-n-d-spaces">Vector similarity in N-D spaces<a hidden class="anchor" aria-hidden="true" href="#vector-similarity-in-n-d-spaces">#</a></h2>
<p>One way to find the similarity between 2 vectors is to determine their <em>scaled dot product</em>, namely the cosine of the angle. It is a stable way of detecting how &ldquo;equal&rdquo; two vectors are in relation to <strong>both</strong> direction and distance. Its values vary from -1 to 1.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> math

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">similarity</span>(v1,v2):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    This is just a dummy example, please dont use it in practice.
</span><span style="color:#e6db74">    compute cosine similarity of v1 to v2: (v1 dot v2)/(||v1||*||v2||)
</span><span style="color:#e6db74">    Also see: https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    sumxx, sumxy, sumyy <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(v1)):
        x <span style="color:#f92672">=</span> v1[i]; y <span style="color:#f92672">=</span> v2[i]
        sumxx <span style="color:#f92672">+=</span> x<span style="color:#f92672">*</span>x
        sumyy <span style="color:#f92672">+=</span> y<span style="color:#f92672">*</span>y
        sumxy <span style="color:#f92672">+=</span> x<span style="color:#f92672">*</span>y
    <span style="color:#66d9ef">return</span> sumxy<span style="color:#f92672">/</span>math<span style="color:#f92672">.</span>sqrt(sumxx<span style="color:#f92672">*</span>sumyy)
    
    
x <span style="color:#f92672">=</span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">41</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">62</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">600</span>]
y <span style="color:#f92672">=</span> [<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>]

<span style="color:#66d9ef">print</span>(similarity(x,y))
</code></pre></div><h2 id="self-attention">Self attention<a hidden class="anchor" aria-hidden="true" href="#self-attention">#</a></h2>
<p>In a nutshell, it is a way to differentiably &ldquo;select/focus on&rdquo; certain parts of the embedding more than the others. I&rsquo;ve written <a href="https://mayukhdeb.github.io/blog/post/what-on-earth-is-attention/">a post where I&rsquo;ve tried to explain how it works from scratch</a>.</p>
<h2 id="short-residual-skip-connections">Short residual skip connections<a hidden class="anchor" aria-hidden="true" href="#short-residual-skip-connections">#</a></h2>
<p>You might know this already but they basically act as &ldquo;information bridges&rdquo; which helps the gradients flow back from deep into the model into the earlier layers very easily. Stuff that makes up the &ldquo;Res&rdquo; in &ldquo;ResNets&rdquo;.</p>
<h2 id="layer-normalization">Layer Normalization<a hidden class="anchor" aria-hidden="true" href="#layer-normalization">#</a></h2>
<p>Let&rsquo;s first try to see whats batch normalization, this will help us understand layer normalization later on.</p>
<p><strong>Batch normalization is not needed here, but you should know how it works</strong></p>
<p>Mathematically, BN layer transforms each input in the current mini-batch by:</p>
<ol>
<li>Subtracting the input mean in the current mini-batch</li>
<li>Dividing it by the standard deviation.</li>
</ol>
<p>But each layer doesn’t need to expect inputs with zero mean and unit variance, but instead, probably the <strong>model might perform better with some other mean and variance</strong>. Hence the BN layer also introduces two learnable parameters γ and β (gamma and beta).</p>
<p>The important thing to note here is that this normalization works <strong>along the batch dim</strong>.</p>
<p><strong>ok so how is layer normalization different from batch normalization ?</strong></p>
<p>The only difference is that layer normalization individually (<strong>along the feature axes</strong>) normalizes each feature to have zero mean and unit variance.</p>
<h2 id="multi-head-attention">Multi-head attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">#</a></h2>
<p>Multi-head attention is just multiple attention blocks in one single layer whose outputs are concatenated later after the forward passes. The main advantage is that it allows us to attend to different parts of the sequence differently on each &ldquo;head&rdquo;.</p>
<blockquote>
<p>In caveman words: multi head attention is just a &ldquo;bunch&rdquo; of attention mechanisms in a single forward pass</p>
</blockquote>
<figure>
    <img src="https://github.com/Mayukhdeb/blog/blob/master/content/post/images/2021_aug_10/multi_head_attention.png?raw=true" width="100%"/> 
</figure>

<h2 id="further-reading">Further reading:<a hidden class="anchor" aria-hidden="true" href="#further-reading">#</a></h2>
<ol>
<li>
<p>The vision transformer: <a href="https://theaisummer.com/vision-transformer/">https://theaisummer.com/vision-transformer/</a></p>
</li>
<li>
<p>An image is worth 16x16 words, explained: <a href="https://www.youtube.com/watch?v=TrdevFK_am4">https://www.youtube.com/watch?v=TrdevFK_am4</a></p>
</li>
</ol>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mayukhdeb.github.io/blog/tags/deep-learning/">Deep learning</a></li>
    </ul>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://mayukhdeb.github.io/blog/">@mayukhdeb</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<button class="top-link" id="top-link" type="button" aria-label="go to top" title="Go to Top (Alt + G)" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <path d="M12 6H0l6-6z" /></svg>
</button>



<script defer src="https://mayukhdeb.github.io/blog/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    mybutton.onclick = function () {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
        window.location.hash = ''
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
