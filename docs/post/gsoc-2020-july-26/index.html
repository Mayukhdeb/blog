<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Training a GAN | @mayukhdeb</title>

<meta name="keywords" content="GSoC, OpenWorm, deep learning, GAN" />
<meta name="description" content="Google summer of code week 10">
<meta name="author" content="">
<link rel="canonical" href="https://mayukhdeb.github.io/blog/post/gsoc-2020-july-26/" />
<link href="https://mayukhdeb.github.io/blog/assets/css/stylesheet.min.d1bc2b736056bd5698d770eeedc08a73bce9e6cebb30810f6f1b2c2048e46ab8.css" integrity="sha256-0bwrc2BWvVaY13Du7cCKc7zp5s67MIEPbxssIEjkarg=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://mayukhdeb.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://mayukhdeb.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mayukhdeb.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://mayukhdeb.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://mayukhdeb.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.68.3" />


<meta property="og:title" content="Training a GAN" />
<meta property="og:description" content="Google summer of code week 10" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mayukhdeb.github.io/blog/post/gsoc-2020-july-26/" />
<meta property="article:published_time" content="2020-07-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-07-26T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Training a GAN"/>
<meta name="twitter:description" content="Google summer of code week 10"/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training a GAN",
  "name": "Training a GAN",
  "description": "Google Summer of Code | Coding period week 8",
  "keywords": [
    "GSoC", "OpenWorm", "deep learning", "GAN"
  ],
  "articleBody": "  Training a GAN is an act of balance where we have to balance the generator and the discriminator in a game of losses and probabilities.\nThe generator: The generator, when properly trained, takes in random noise as input and returns a sample which would look very similar to the ones from the training distribution. What we’re doing here is basically mapping points a high dimensional distribution to the training samples.\nWhile training, the generator’s main objective is to try to fool the discriminator by generating real looking images.\nThe discriminator: The discriminator, when properly trained, would be able to take in an image as an input and return a probability as to whether the image is real or generated. This is more of a classification problem.\nWhile training, the discriminator’s main objective is to distinguish between real and fake images.\nInitially when the training starts, both the models are equally bad at their jobs, but in the end pf training, the ideal scenario would be when the generator generators images in-distinguishable from the training set. While the discriminator takes in thatgenerated image and returns a probability of 0.5.\nThe game of minimax   Let’s break down the equation above part by part:   x is the real data\n  z is the latent vector i.e random noise. It can also be seen as a point in the latent space with n dimensions. For example, torch.randn(64, 1, 1) is a point in the 64 dimensional space.\n  D(x) is the probability of the original image being real according to the discriminator.\n  G(z) is the generated image with noise z as input.\n  D(G(z)) is the probability of the generated image being real according to the discriminator.\n  E represents the expected value function, the expected value is also known as the expectation, mathematical expectation, mean, average, or first moment.\n  So what’s going on during training ? The discriminator wants to maximize D(x) and minimize D(G(z))\nThe generator wants to maximize D(G(z)) so that the discriminator gets fooled into believing that the generated image is real.\nThe key factor to maintaining a balance between the 2 model’s capabilities is to choose the right loss landscape.\nThe training loop: 1. Update discriminator : maximize log(D(x)) + log(1 - D(G(z)))  1.1 Backprop the discriminator after feeding real images with labels as 1 (sometimes people also use noisy labels with 1 ± noise) (trying to make D(x) tend towards 1)\n  1.2 Generate fake images (G(z)) with the generator with random noise z as input.\n  1.2 Backprop the discriminator after feeding fake images with labels as 0 (sometimes people also use noisy labels with 0 ± noise)\n  Note: the real and the fake batches were not mixed, first we fed the real batch, and then the fake batch.\n 2. Update generator: maximize log(D(G(z)))  2.1 Use the discriminator’s output on the fake images to calculate a loss with respect to 1. Since the generator’s job is to try to make the discriminator return a 1 on fake images, the closer criterion(discriminator_output, one) is to zero, the better the situation is for the generator.\n If you’re more into code than human sentences, here’s a snippet\nlabel.fill_(real_label) # perform another forward pass of all-fake batch through D output = netD(fake_images).view(-1) # Calculate G's loss based on this output errG = criterion(output, label) # Calculate gradients for G errG.backward() D_G_z2 = output.mean().item() # Update G optimizerG.step() If you made it this far, here’s a gif of another model trying to make predictions on the generated images:\n  ",
  "wordCount" : "580",
  "inLanguage": "en",
  "datePublished": "2020-07-26T00:00:00Z",
  "dateModified": "2020-07-26T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mayukhdeb.github.io/blog/post/gsoc-2020-july-26/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "@mayukhdeb",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mayukhdeb.github.io/blog/favicon.ico"
    }
  }
}
</script>



</head>

<body class="">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mayukhdeb.github.io/blog/" accesskey="h" title="@mayukhdeb (Alt + H)">@mayukhdeb</a>
            <span class="logo-switches">
                <span class="theme-toggle" title="(Alt + T)">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()"></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      Training a GAN
    </h1>
    <div class="post-meta">July 26, 2020

    </div>
  </header> 

  <div class="post-content">
<figure>
    <img src="https://raw.githubusercontent.com/devoworm/GSoC-2020/master/Pre-trained%20Models%20%28DevLearning%29/images/generated_embryos_3.gif" width="50%"/> 
</figure>

<p>Training a GAN is an act of balance where we have to balance the generator and the discriminator in a game of losses and probabilities.</p>
<h2 id="the-generator">The generator:<a hidden class="anchor" aria-hidden="true" href="#the-generator">#</a></h2>
<p>The generator, when properly trained, takes in random noise as input and returns a sample which would look very similar to the ones from the training distribution. What we&rsquo;re doing here is basically mapping points a high dimensional distribution to the training samples.</p>
<p>While training, the generator&rsquo;s main objective is to <strong>try to fool the discriminator</strong> by generating real looking images.</p>
<h2 id="the-discriminator">The discriminator:<a hidden class="anchor" aria-hidden="true" href="#the-discriminator">#</a></h2>
<p>The discriminator, when properly trained, would be able to take in an image as an input and return a probability as to whether the image is real or generated. This is more of a classification problem.</p>
<p>While training, the discriminator&rsquo;s main objective is to <strong>distinguish between real and fake images</strong>.</p>
<p>Initially when the training starts, both the models are equally bad at their jobs, but in the end pf training, the ideal scenario would be when the generator generators images in-distinguishable from the training set. While the discriminator takes in thatgenerated image and returns a probability of 0.5.</p>
<h2 id="the-game-of-minimax">The game of minimax<a hidden class="anchor" aria-hidden="true" href="#the-game-of-minimax">#</a></h2>
<figure>
    <img src="https://raw.githubusercontent.com/Mayukhdeb/blog/master/content/post/images/july_26/minimax.png" width="50%"/> 
</figure>

<h3 id="lets-break-down-the-equation-above-part-by-part">Let&rsquo;s break down the equation above part by part:<a hidden class="anchor" aria-hidden="true" href="#lets-break-down-the-equation-above-part-by-part">#</a></h3>
<ul>
<li>
<p><code>x</code> is the real data</p>
</li>
<li>
<p><code>z</code> is the latent vector i.e random noise. It can also be seen as a point in the latent space with n dimensions.  For example, <code>torch.randn(64, 1, 1)</code> is a point in the 64 dimensional space.</p>
</li>
<li>
<p><code>D(x)</code> is the probability of the original image being real according  to the discriminator.</p>
</li>
<li>
<p><code>G(z)</code> is the generated image with noise <code>z</code> as input.</p>
</li>
<li>
<p><code>D(G(z))</code> is the probability of the generated image being real according to the discriminator.</p>
</li>
<li>
<p><code>E</code> represents the expected value function, the expected value is also known as the expectation, mathematical expectation, mean, average, or first moment.</p>
</li>
</ul>
<h3 id="so-whats-going-on-during-training-">So what&rsquo;s going on during training ?<a hidden class="anchor" aria-hidden="true" href="#so-whats-going-on-during-training-">#</a></h3>
<p>The discriminator wants to <strong>maximize <code>D(x)</code> and minimize <code>D(G(z))</code></strong></p>
<p>The generator wants to <strong>maximize <code>D(G(z))</code></strong> so that the discriminator gets fooled into believing that the generated image is real.</p>
<p>The key factor to maintaining a balance between the 2 model&rsquo;s capabilities is to choose the right loss landscape.</p>
<h2 id="the-training-loop">The training loop:<a hidden class="anchor" aria-hidden="true" href="#the-training-loop">#</a></h2>
<h3 id="1-update-discriminator--maximize-logdx--log1---dgz">1. Update discriminator : maximize <code>log(D(x)) + log(1 - D(G(z)))</code><a hidden class="anchor" aria-hidden="true" href="#1-update-discriminator--maximize-logdx--log1---dgz">#</a></h3>
<blockquote>
<p>1.1 Backprop the discriminator after feeding real images with labels as 1 (sometimes people also use noisy labels with <code>1 ± noise</code>) (trying to make <code>D(x)</code> tend towards 1)</p>
</blockquote>
<blockquote>
<p>1.2 Generate fake images (<code>G(z)</code>) with the generator with random noise <code>z</code> as input.</p>
</blockquote>
<blockquote>
<p>1.2 Backprop the discriminator after feeding fake images with labels as 0 (sometimes people also use noisy labels with <code>0 ± noise</code>)</p>
</blockquote>
<blockquote>
<p>Note: the real and the fake batches were not mixed, first we fed the real batch, and then the fake batch.</p>
</blockquote>
<h3 id="2-update-generator-maximize-logdgz">2. Update generator: maximize <code>log(D(G(z)))</code><a hidden class="anchor" aria-hidden="true" href="#2-update-generator-maximize-logdgz">#</a></h3>
<blockquote>
<p>2.1 Use the discriminator&rsquo;s output on the fake images to calculate a loss with respect to 1. Since the generator&rsquo;s job is to try to make the discriminator return a 1 on fake images, the closer <code>criterion(discriminator_output, one)</code> is to zero, the better the situation is for the generator.</p>
</blockquote>
<p>If you&rsquo;re more into code than human sentences, here&rsquo;s a snippet</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">label<span style="color:#f92672">.</span>fill_(real_label) <span style="color:#75715e"># perform another forward pass of all-fake batch through D</span>
output <span style="color:#f92672">=</span> netD(fake_images)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
<span style="color:#75715e"># Calculate G&#39;s loss based on this output</span>
errG <span style="color:#f92672">=</span> criterion(output, label)
<span style="color:#75715e"># Calculate gradients for G</span>
errG<span style="color:#f92672">.</span>backward()
D_G_z2 <span style="color:#f92672">=</span> output<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item()
<span style="color:#75715e"># Update G</span>
optimizerG<span style="color:#f92672">.</span>step()
</code></pre></div><p>If you made it this far, here&rsquo;s a gif of another model trying to make predictions on the generated images:</p>
<figure>
    <img src="https://raw.githubusercontent.com/devoworm/GSoC-2020/master/Pre-trained%20Models%20%28DevLearning%29/images/model_vs_GAN.gif" width="70%"/> 
</figure>


</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://mayukhdeb.github.io/blog/tags/deep-learning/">Deep learning</a></li>
      <li><a href="https://mayukhdeb.github.io/blog/tags/gsoc/">GSoC</a></li>
      <li><a href="https://mayukhdeb.github.io/blog/tags/openworm/">OpenWorm</a></li>
      <li><a href="https://mayukhdeb.github.io/blog/tags/gan/">GAN</a></li>
    </ul>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://mayukhdeb.github.io/blog/">@mayukhdeb</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<button class="top-link" id="top-link" type="button" aria-label="go to top" title="Go to Top (Alt + G)" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <path d="M12 6H0l6-6z" /></svg>
</button>



<script defer src="https://mayukhdeb.github.io/blog/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    mybutton.onclick = function () {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
        window.location.hash = ''
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
