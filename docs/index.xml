<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>@mayukhdeb on @mayukhdeb </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>https://mayukhdeb.github.io/blog/</link>
    <language>en-us</language>
    
    
    <updated>Sun, 01 Mar 2020 00:00:00 UTC</updated>
    
    <item>
      <title>Deep chicken terminator</title>
      <link>https://mayukhdeb.github.io/blog/post/deep-chicken-terminator/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/deep-chicken-terminator/</guid>
      <description>&lt;h1 id=&#34;deep-chicken-terminator&#34;&gt;deep-chicken-terminator&lt;/h1&gt;
&lt;p&gt;deep learning to track (and possibly kill)  chicken in minecraft&lt;/p&gt;
&lt;h2 id=&#34;step-1---collecting-training-data-for-the-deep-neural-network&#34;&gt;Step 1 - collecting training data for the deep neural network&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/training_data_samples.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was mostly just me taking cropped screenshots of animals while playing the game, took about 40 screenshots of each animal.&lt;/li&gt;
&lt;li&gt;The 20 screenshots of each animal were then augmented and got 500 samples of each.&lt;/li&gt;
&lt;li&gt;The dataset is very small, but it works anyways for now&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2---training-a-deep-learning-model-on-the-samples&#34;&gt;Step 2 - training a deep learning model on the samples&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/training_curve.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The architecture was kept intentionally small so that it keeps a good response time on the live feed&lt;/li&gt;
&lt;li&gt;The dataset had only 2000 images sized at 50*50, so training barely took any time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3---detecting-a-chicken-or-any-animal-for-that-matter-from-an-image&#34;&gt;Step 3 - detecting a chicken (or any animal for that matter) from an image&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/pre_barbecue.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was done using a custom &lt;code&gt;detect_animal_numpy()&lt;/code&gt; function which iterates through the image with a certain kernel size and a stride size, and feeds each sample to the trained NN (nicknamed hunter)&lt;/li&gt;
&lt;li&gt;A heatmap is then generated from the output of the NN which gives us a probability distribution over the image of a certain animal ( chicken, pig, or panda)&lt;/li&gt;
&lt;li&gt;why use heatmaps instead of rectangles ? because they look cooler.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/barbecue.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adventures with augmentation</title>
      <link>https://mayukhdeb.github.io/blog/post/adventures-with-augmentation/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/adventures-with-augmentation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/Mayukhdeb/adventures-with-augmentation/tree/master/&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/bfeb5472ee3df9b7c63ea3b260dc0c679be90b97/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656e6465722d6e627669657765722d6f72616e67652e7376673f636f6c6f72423d66333736323626636f6c6f72413d346434643464&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;parallel-cnns-work-just-as-good-as-they-look&#34;&gt;Parallel CNNs work just as good as they look&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/parallel_cnn.jpg&#34; alt=&#34;parallel_cnns&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;but-why-use-them-anyways-&#34;&gt;But why use them anyways ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Because when two different architectures are trained on the same training set, they don&amp;rsquo;t have the same weaknesses (i.e different confusion matrices)&lt;/li&gt;
&lt;li&gt;This means that when both are combined, they tend to neutralise each other&amp;rsquo;s weaknesses, which gives us a boost in accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;class-activation-heatmaps-on-deep-neural-networks&#34;&gt;Class activation heatmaps on deep neural networks&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/original_comma.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/heatmap.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/superimposed.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shows the regions of the image which gave the most activations for a certain label in a trained classification model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In simpler words, it tells us about the regions of the image which made the model decide that it belongs to a certain label &amp;ldquo;x&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/cell_detect_pipeline.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;And when the heatmap is superimposed upon the real image, it gives us an insight on how the CNN &amp;ldquo;looked&amp;rdquo; at the image&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Mind the bend</title>
      <link>https://mayukhdeb.github.io/blog/post/mind_the_bend/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/mind_the_bend/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/example.gif&#34; alt=&#34;example&#34;&gt;
&lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;The whole point of this project was to make a car stay on the road all by itself and defeat other players in a racing game called speed dreams.&lt;/p&gt;
&lt;h2 id=&#34;collection-of-training-data&#34;&gt;Collection of training data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rapidly takes screenshots of the game and saves them in a folder&lt;/li&gt;
&lt;li&gt;The label in this case is the x-co-ordinate of the mouse which is captured by pyautogui and is stored in the formatted filename of each collected image. Image fileame formatting is done as  &lt;code&gt;(x-value)_(unique_ID).png&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;processing-images&#34;&gt;Processing images&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Converts all images to numpy arrays with a depth of 3 for R,G and B color channels&lt;/li&gt;
&lt;li&gt;The shape gets changed from  &lt;code&gt;[ width, height, depth ]&lt;/code&gt; to &lt;code&gt;[ depth, width, height]&lt;/code&gt; for it to be of the right size for the CNN input channel&lt;/li&gt;
&lt;li&gt;Pairs them with the corresponding mouse x-coordinate as array &lt;code&gt;[  [[[image]]]  ,  [x-value]   ]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;all of the generated pairs are then stacked into one numpy array with the help of np.vstack() and saved&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-preprocessing-and-augmentation&#34;&gt;Data preprocessing and augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First things first, plotted the frequency distribution of each steering value hen&lt;/li&gt;
&lt;li&gt;Roughly doubled the amount of training data by generating mirror images of existing images and vstacking them with reversed steer value.&lt;/li&gt;
&lt;li&gt;Flattened the frequency distribution by making copies of random ramples of under-represented steering value data points&lt;/li&gt;
&lt;li&gt;Normalised the steering values by replacing the x-co-ordinates with steering values. In my case the &amp;ldquo;straight&amp;rdquo; steer value was at &lt;code&gt;x = 400&lt;/code&gt;, for &lt;code&gt;normalised_value = 400 - x_value&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;note :  Right is negative steer value and left is positive&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;self-driving&#34;&gt;Self driving&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rapidly taken screenshots are prerpocessed and fed to the trained CNN drunk_driver()&lt;/li&gt;
&lt;li&gt;&lt;code&gt;drunk_driver()&lt;/code&gt; returns a steer value&lt;/li&gt;
&lt;li&gt;Returned value is taken care  of by pyauogui which moves the mouse accordingly&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
