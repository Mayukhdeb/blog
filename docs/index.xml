<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>@mayukhdeb on @mayukhdeb </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>https://mayukhdeb.github.io/blog/</link>
    <language>en-us</language>
    
    
    <updated>Thu, 11 Jun 2020 00:00:00 UTC</updated>
    
    <item>
      <title>PCA and semantic segmentation</title>
      <link>https://mayukhdeb.github.io/blog/post/gsoc-2020-june-14/</link>
      <pubDate>Thu, 11 Jun 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/gsoc-2020-june-14/</guid>
      <description>&lt;p&gt;Week 2 has been the most satisfying week in GSoC yet, with a lot of things getting done. Let&amp;rsquo;s have a look at them one at a time:&lt;/p&gt;
&lt;h2 id=&#34;starting-out-with-semantic-segmentationhttpsnbviewerjupyterorggithubdevowormgsoc-2020treemasterpre-trained20models2028devlearning29notebooksembryo_segmentation&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/tree/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/embryo_segmentation/&#34;&gt;Starting out with semantic segmentation&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is one of the primary objectives of my GSoC project, and it&amp;rsquo;s about time that I started working on it.&lt;/p&gt;
&lt;p&gt;The primary data source would be the &lt;a href=&#34;https://www.wormimage.org/&#34;&gt;WormImage Database&lt;/a&gt;, and the objective is to train a model which can segment out the nuclei from the  image of an embryo of the C. elegans worm.&lt;/p&gt;
&lt;p&gt;The problem is that there&amp;rsquo;s no labelled segmentation data, only raw images. But I&amp;rsquo;ve been trying to figure out a way through it, which involves:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Taking an original image and manually drawing over the desired parts which are to be extracted. I had to come up with a &lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20(DevLearning)/notebooks/embryo_segmentation/drawing_tool_for_manual_labelling.ipynb&#34;&gt;simple drawing tool&lt;/a&gt; which could be used to generate the training masks.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_14/drawing.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Run both the original image and the mask through a heavy (but practical) &lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/embryo_segmentation/augmentation_prototyping.ipynb&#34;&gt;image augmentation pipeline&lt;/a&gt; (generate at least 50 images from one)&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_14/augment.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Repeat steps 1 and 2 for a few images to get a larger dataset to train on.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I already did steps 1 and 2 on one such image of an embryo and generated a few augmented samples as seen above.&lt;/p&gt;
&lt;h2 id=&#34;principal-component-analysis-on-embryo-metadatahttpsnbviewerjupyterorggithubdevowormgsoc-2020blobmasterpre-trained20models2028devlearning29notebooksembryo_analysisprincipal_component_analysisipynb&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/embryo_analysis/principal_component_analysis.ipynb&#34;&gt;Principal component analysis on embryo metadata&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is where I used some tools like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;StandardScaler().fit_transform(x)&lt;/code&gt; standardizes the features by removing the mean and scaling them to unit variance. In simpler words, it will transform the data such that its distribution will have a mean value 0 and standard deviation of 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;sklearn.decomposition.PCA(n_components = 2)&lt;/code&gt; helped in decomposing the 4 dimensional data ( &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;time&lt;/code&gt;) into a 2 dimensional space while conserving as much information as possible.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even though 100% of the information from the 4D space could not be projected into the 2 principal components, it gave us a good idea on how cells with similar (or same) lineage ancestors lied closer to each other in the 2D space composed of the 2 principal components.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_14/PCA.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>The &#34;EPIC&#34; dataset</title>
      <link>https://mayukhdeb.github.io/blog/post/gsoc-2020-june-7/</link>
      <pubDate>Sat, 06 Jun 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/gsoc-2020-june-7/</guid>
      <description>&lt;h2 id=&#34;exploring-the-metadatahttpsnbviewerjupyterorggithubdevowormgsoc-2020blobmasterpre-trained20models2028devlearning29notebooksembryo_analysismetadata_explorationipynb&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/embryo_analysis/metadata_exploration.ipynb&#34;&gt;Exploring the metadata&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;So, the week started with me shifting my focus from movement analysis to developmental biology. Which made me stumble upon the &lt;a href=&#34;http://epic.gs.washington.edu/&#34;&gt;EPIC dataset&lt;/a&gt;. The website contained a lot of videos capturing the process of embryogenesis in the C. elegans embryo.  The best part was that each video was accompanied with time tagged metadata with columns like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; co-ordinates of the cell&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cell name (more on that below)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Time value of when the data point was captured.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plotting the movements of some of the cells gave an interesting insight on how a certain type of a dwells in a certain part of the embryo.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_7/movement_vis.png&#34; width=&#34;40%&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;deep-learning-on-the-metadatahttpsnbviewerjupyterorggithubdevowormgsoc-2020blobmasterpre-trained20models2028devlearning29notebooksembryo_analysisdeep_learning_on_metadataipynb&#34;&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/embryo_analysis/deep_learning_on_metadata.ipynb&#34;&gt;Deep learning on the metadata&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The first objective was to make sense of what the cell names meant, and the diagram shown below summed it up pretty well.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_7/lineage_diagram.jpg&#34; width=&#34;40%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;But there was a new problem now, it was that there were more than 700 unique cell names with a highly unbalanced distribution through the dataset.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_7/compact_cellname_explain.png&#34; width=&#34;40%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The solution was to &amp;ldquo;shorten&amp;rdquo; the cell names to only the first few characters. This would give us info about the cell&amp;rsquo;s lineage (or the name of the cell itself, depending on the value of &lt;code&gt;time&lt;/code&gt;) and not necessarily it&amp;rsquo;s precise name. This shortened &lt;code&gt;compact_cellname&lt;/code&gt; would now be used as a label for the deep learning model.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_7/pie.png&#34; width=&#34;30%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;This reduced the number of labels to 32, but there was still a lot of imbalance (as seen above), which was taken care of by the &lt;a href=&#34;https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html&#34;&gt;&lt;code&gt;RandomOverSampler()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The next step was to train a model on the data, and with the help of an adaptive learning rate which only reduced when the loss plateaued (check - &lt;a href=&#34;https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau&#34;&gt;&lt;code&gt;ReduceLROnPlateau()&lt;/code&gt;&lt;/a&gt;), the simple 5 layer model reached a a 90% accuracy after 30 epochs of training.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_7/lr.png&#34; width=&#34;40%&#34;/&gt; 
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/june_7/training_curve.png&#34; width=&#34;40%&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The model can obviously be improved in the coming weeks with more data and a better architecture.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Finding patterns</title>
      <link>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-31/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-31/</guid>
      <description>&lt;p&gt;Week 3 started off with some more improvements to the &amp;ldquo;skeletonization process&amp;rdquo;, but soon took a turn towards data analysis. I downloaded some videos from the movement database to the runtime and saved CSV files containing the distances between the head and tail of the worm in form of a time-series.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_31/head_tail_time_series_data.png&#34; width=&#34;100%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;As seen above, there are two distinct patterns that are observed in the head to tail distances of the &amp;ldquo;wild&amp;rdquo; strain and the &amp;ldquo;unc&amp;rdquo; strain of the worm. The wild type seems to be more noisy with deeper local minimas. While the changes in the &amp;ldquo;unc&amp;rdquo; type are more gradual and predictable.&lt;/p&gt;
&lt;p&gt;It is possible to generate a large amount of time series data from the videos that are available on the Movement database. This time series data can be used for 2 possible purposes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First would be to use an LSTM to predict the future sequence of values, a good enough LSTM would be able to find the underlying patterns through the noise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Another possibility would be to use the LSTM to classify the worm&amp;rsquo;s strain based on the time series data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I already used a simple LSTM RNN on the time series data as a proof of concept, and here&amp;rsquo;s how the predictions looked like:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_31/LSTM_sample_alpha_001.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The predictions made aren&amp;rsquo;t absolutely perfect for now because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The dummy model is too small, but it still manages to predict the first quarter of test zone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The amount of data used to train the model is relatively low for faster prototyping.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trying to achieve at least one of these goals would be the goal of the next week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Movement Database</title>
      <link>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-24/</link>
      <pubDate>Sun, 24 May 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-24/</guid>
      <description>&lt;p&gt;After week 1, the new priority was to find the right data on the internet. I started the week off by reading papers based on locomotion in the C. elegans worm. Surprisingly enough (or unsurprisingly, depending on how much you&amp;rsquo;re into worms), most of the data was from the &lt;a href=&#34;http://movement.openworm.org/&#34;&gt;OpenWorm Movement Database&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Almost all the video data on the database is from the &lt;a href=&#34;https://github.com/ver228/tierpsy-tracker&#34;&gt;Tierpsy tracker&lt;/a&gt;, which is an amazing worm tracking tool in itself. But there was a new hurdle now.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/initial_image.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The issue with the video data (as seen above) wThe issue with the video data (as seen above) was that the worms were &amp;ldquo;partially segmented out&amp;rdquo;. Which meant that the background was all blacked out except for a small &amp;ldquo;padding area&amp;rdquo; around the worm. This &amp;ldquo;padding area&amp;rdquo; was a good safety measure on the Tierpsy tracker&amp;rsquo;s side, but was a minor hurdle  for someone who wanted to segment out just the worm from the image in order to gain inferences from behavioural dynamics.&lt;/p&gt;
&lt;p&gt;This hurdle was taken care of in 3 stages:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/OW_movement_database_pipeline.png&#34; width=&#34;100%&#34;/&gt; 
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Stage 1: Extracting the &amp;ldquo;padding region&amp;rdquo; around the worm with &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html&#34;&gt;&lt;code&gt;cv2.threshold()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stage 2: Removing the padding region by using &lt;a href=&#34;https://docs.opencv.org/master/d0/d86/tutorial_py_image_arithmetics.html&#34;&gt;&lt;code&gt;cv2.bitwise_and()&lt;/code&gt;&lt;/a&gt; between the real and the thresholded image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Removing any extra noise (as seen on stage 2) with &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html&#34;&gt;&lt;code&gt;cv2.erode()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now the next objecive was to extract the head and tail, this was done  using corner detection with &lt;a href=&#34;https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html#goodfeaturestotrack&#34;&gt;&lt;code&gt;cv2.goodFeaturesToTrack()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;maxCorners = 2&lt;/code&gt; to only capture the 2 most prominent corners.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/head_and_tail.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;To complete the skeletonization, all that was needed now was to skeletonize the main body, and that was done with the very self explanatory function named &lt;a href=&#34;https://scikit-image.org/docs/dev/auto_examples/edges/plot_skeleton.html&#34;&gt;&lt;code&gt;skimage.morphology.skeletonize()&lt;/code&gt;&lt;/a&gt;. Combined with two circles which signify the head and the tail, we get a clean skeleton which can now be used for analysis on a bigger scale with a large number of frames.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/skeleton.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/skeleton_long.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;
as that the worms were &amp;ldquo;partially segmented out&amp;rdquo;. Which meant that the background was all blacked out except for a small &amp;ldquo;padding area&amp;rdquo; around the worm. This &amp;ldquo;padding area&amp;rdquo; was a good safety measure on the Tierpsy tracker&amp;rsquo;s side, but was a minor hurdle  for someone who wanted to segment out just the worm from the image in order to gain inferences from behavioural dynamics.&lt;/p&gt;
&lt;p&gt;This hurdle was taken care of in 3 stages:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/OW_movement_database_pipeline.png&#34; width=&#34;100%&#34;/&gt; 
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Stage 1: Extracting the &amp;ldquo;padding region&amp;rdquo; around the worm with &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html&#34;&gt;&lt;code&gt;cv2.threshold()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stage 2: Removing the padding region by using &lt;a href=&#34;https://docs.opencv.org/master/d0/d86/tutorial_py_image_arithmetics.html&#34;&gt;&lt;code&gt;cv2.bitwise_and()&lt;/code&gt;&lt;/a&gt; between the real and the thresholded image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Removing any extra noise (as seen on stage 2) with &lt;a href=&#34;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html&#34;&gt;&lt;code&gt;cv2.erode()&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now the next objecive was to extract the head and tail, this was done  using corner detection with &lt;a href=&#34;https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html#goodfeaturestotrack&#34;&gt;&lt;code&gt;cv2.goodFeaturesToTrack()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;maxCorners = 2&lt;/code&gt; to only capture the 2 most prominent corners.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/head_and_tail.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;To complete the skeletonization, all that was needed now was to skeletonize the main body, and that was done with the very self explanatory function named &lt;a href=&#34;https://scikit-image.org/docs/dev/auto_examples/edges/plot_skeleton.html&#34;&gt;&lt;code&gt;skimage.morphology.skeletonize()&lt;/code&gt;&lt;/a&gt;. Combined with two circles which signify the head and the tail, we get a clean skeleton which can now be used for analysis on a bigger scale with a large number of frames.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/skeleton.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_24/skeleton_long.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello Openworm</title>
      <link>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-17/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-17/</guid>
      <description>&lt;p&gt;Ten weeks of proposal writing and 3 personal (but very beloved) deep learning projects later, I was selected for the only project I worked towards for GSoC, which was &lt;a href=&#34;https://summerofcode.withgoogle.com/projects/#6078813261266944&#34;&gt;Pre-trained models for developmental neuroscience&lt;/a&gt; under &lt;a href=&#34;https://www.incf.org/&#34;&gt;INCF&lt;/a&gt;. And things have been better than ever since then.&lt;/p&gt;
&lt;p&gt;The first week was mostly me getting to know more about the OpenWorm community and what are they striving to do. I got to meet &lt;a href=&#34;https://github.com/balicea&#34;&gt;Dr. Bradley Alicea&lt;/a&gt;, &lt;a href=&#34;https://github.com/nvinayvarma189&#34;&gt;Vinay Varma&lt;/a&gt; and &lt;a href=&#34;https://github.com/ujjwalll&#34;&gt;Ujjwal Singh&lt;/a&gt;. They gave me a warm welcome into the community.&lt;/p&gt;
&lt;p&gt;I started setting up the environment right after the results were out, and cleaned up one of my &lt;a href=&#34;https://github.com/Mayukhdeb/adventures-with-augmentation&#34;&gt;older repos&lt;/a&gt; which would act as a quick reference through the span of this project.&lt;/p&gt;
&lt;p&gt;The first weekly meeting with OpenWorm gave me a much better idea of the goals of this project.&lt;/p&gt;
&lt;p&gt;The key to a good deep learning model is good data. If the data is clean, half the battle is won. This led to the the first &lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/worm_tracking_pipeline.ipynb&#34;&gt;notebook&lt;/a&gt; which builds a pipeline for tracking and segmenting the worms directly from video files. The pipeline works as follows:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/may_17_visualization.png&#34; height=&#34;900&#34;/&gt; 
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The video file is first trimmed to shorten the clip to extract only a short subclip (this is optional).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All the frames from the subclip are extracted from the video using &lt;a href=&#34;https://zulko.github.io/moviepy/ref/ffmpeg.html&#34;&gt;ffmpeg&lt;/a&gt; and saved into a folder in a numerical sequence of filenames.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The locations of the worms are determined by drawing padded rectangles around the contours of the thresholded image.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/rectangles.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;The original image is then cropped and segmented to remove all background except the worm(s) in the image.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/segmented.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Here&amp;rsquo;s a GIF to visualise the tracking on the live feed.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/tracking.gif&#34; width=&#34;80%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Apart from all of this, I also read quite a few papers on the C.elegans worm. This was an effort towards getting more familiar with the underlying biological concepts surrounding the worm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep chicken saviour</title>
      <link>https://mayukhdeb.github.io/blog/post/deep-chicken-saviour/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/deep-chicken-saviour/</guid>
      <description>&lt;h2 id=&#34;before&#34;&gt;Before:&lt;/h2&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/no_FGSM.gif&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;after&#34;&gt;After:&lt;/h2&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/FGSM.gif&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;adversarial-blocks-generated-by-negative-epsilon-fgsm-to-confuse-the-traned-deep-neural-network&#34;&gt;Adversarial blocks generated by negative epsilon FGSM to confuse the traned deep neural network&lt;/h2&gt;
&lt;p&gt;These blocks are made by starting out with a dark grey image and then backpropagating on the image with the pre trained network with a negative epsilon in order to minimise loss for the target class, a more negative epsilon will not necessarily give a better result. But it&amp;rsquo;s a bell curve instead, and the epsilon is optimized by looking for the local target class probability maxima in the domain [&lt;code&gt;lower_limit&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;These adversarial blocks can be generated for any animal class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/patch_to_block.png&#34; alt=&#34;patch_to_block&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/adv_block.gif&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;fast-gradient-sign-method-chart_with_upwards_trend&#34;&gt;Fast Gradient Sign Method :chart_with_upwards_trend:&lt;/h2&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sign(data_gradients)&lt;/code&gt; gives the element wise signs of the data gradient&lt;/li&gt;
&lt;li&gt;&lt;code&gt;epsilon&lt;/code&gt; defines the &amp;ldquo;strength&amp;rdquo; of the perturbation of the image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a nutshell, instead of &lt;strong&gt;optimizing the model to reduce the loss&lt;/strong&gt;, we&amp;rsquo;re &lt;strong&gt;un-optimizing the input image to maximise loss&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This works primarily because of the piecewise linear nature of deep neural networks. For example, look at ReLU or at maxout functions, they&amp;rsquo;re all piecewise linear. Even a carefully tuned sigmoid has an approximate linear nature when taken piecewise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With varying values of epsilon, we will see an approximately linear relationship between &amp;ldquo;confidence&amp;rdquo; and epsilon.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;negative-epsilon-fgsm&#34;&gt;Negative epsilon FGSM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;this can be used to turn one animal into another specific animal for the deep neural network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/negative_eps_FGSM.png&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-are-the-images-optimized-using-the-same-fgsm-&#34;&gt;How are the images &amp;ldquo;optimized&amp;rdquo; using the same FGSM ?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The key here is to understand how FGSM actually worked.
In FGSM, we were tampering with the pixels which has a &lt;em&gt;positive&lt;/em&gt; gradient and added a certain value &lt;code&gt;gradient * epsilon&lt;/code&gt; to each of those pixels. This made the image deviate further and further away from the class it actually belongs to and thus maximising loss in the process. Note that this was done with a &lt;strong&gt;positive epsilon&lt;/strong&gt; value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But for our current objective, we will try to &amp;ldquo;optimize&amp;rdquo; the image to a different class. This can be done by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doing a forward pass with an image of class &lt;code&gt;x&lt;/code&gt; and with a label of &lt;code&gt;y&lt;/code&gt;. Where &lt;code&gt;y&lt;/code&gt; is the class to which we want to convert our image to.&lt;/li&gt;
&lt;li&gt;Performing a backpropagation on the network and extracting the gradients on the input image.&lt;/li&gt;
&lt;li&gt;Now instead of trying to maximise loss using the FGSM, we&amp;rsquo;ll reduce the loss with a &lt;strong&gt;negative epsilon&lt;/strong&gt; FGSM.&lt;/li&gt;
&lt;li&gt;This will help reduce the loss of the image with respect to the target class &lt;code&gt;y&lt;/code&gt;, and with a sufficiently  negative epsilon value, the image gets mis-classified as the target class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you didn&amp;rsquo;t read the boring stuff above, just remember that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;positive epsilon&lt;/strong&gt; value will &lt;strong&gt;un-optimize&lt;/strong&gt; the image&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;negative epsilon&lt;/strong&gt; value will &lt;strong&gt;optimize&lt;/strong&gt; the image for the given label class&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generating-adversarial-patches-from-black-images&#34;&gt;Generating adversarial patches from black images&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/adversarial_patches.png&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep chicken terminator</title>
      <link>https://mayukhdeb.github.io/blog/post/deep-chicken-terminator/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/deep-chicken-terminator/</guid>
      <description>&lt;h1 id=&#34;deep-chicken-terminator&#34;&gt;deep-chicken-terminator&lt;/h1&gt;
&lt;p&gt;deep learning to track (and possibly kill)  chicken in minecraft&lt;/p&gt;
&lt;h2 id=&#34;step-1---collecting-training-data-for-the-deep-neural-network&#34;&gt;Step 1 - collecting training data for the deep neural network&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/training_data_samples.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was mostly just me taking cropped screenshots of animals while playing the game, took about 40 screenshots of each animal.&lt;/li&gt;
&lt;li&gt;The 20 screenshots of each animal were then augmented and got 500 samples of each.&lt;/li&gt;
&lt;li&gt;The dataset is very small, but it works anyways for now&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2---training-a-deep-learning-model-on-the-samples&#34;&gt;Step 2 - training a deep learning model on the samples&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/training_curve.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The architecture was kept intentionally small so that it keeps a good response time on the live feed&lt;/li&gt;
&lt;li&gt;The dataset had only 2000 images sized at 50*50, so training barely took any time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3---detecting-a-chicken-or-any-animal-for-that-matter-from-an-image&#34;&gt;Step 3 - detecting a chicken (or any animal for that matter) from an image&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/pre_barbecue.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was done using a custom &lt;code&gt;detect_animal_numpy()&lt;/code&gt; function which iterates through the image with a certain kernel size and a stride size, and feeds each sample to the trained NN (nicknamed hunter)&lt;/li&gt;
&lt;li&gt;A heatmap is then generated from the output of the NN which gives us a probability distribution over the image of a certain animal ( chicken, pig, or panda)&lt;/li&gt;
&lt;li&gt;why use heatmaps instead of rectangles ? because they look cooler.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/barbecue.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adventures with augmentation</title>
      <link>https://mayukhdeb.github.io/blog/post/adventures-with-augmentation/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/adventures-with-augmentation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/Mayukhdeb/adventures-with-augmentation/tree/master/&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/bfeb5472ee3df9b7c63ea3b260dc0c679be90b97/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656e6465722d6e627669657765722d6f72616e67652e7376673f636f6c6f72423d66333736323626636f6c6f72413d346434643464&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;parallel-cnns-work-just-as-good-as-they-look&#34;&gt;Parallel CNNs work just as good as they look&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/parallel_cnn.jpg&#34; alt=&#34;parallel_cnns&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;but-why-use-them-anyways-&#34;&gt;But why use them anyways ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Because when two different architectures are trained on the same training set, they don&amp;rsquo;t have the same weaknesses (i.e different confusion matrices)&lt;/li&gt;
&lt;li&gt;This means that when both are combined, they tend to neutralise each other&amp;rsquo;s weaknesses, which gives us a boost in accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;class-activation-heatmaps-on-deep-neural-networks&#34;&gt;Class activation heatmaps on deep neural networks&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/original_comma.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/heatmap.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/superimposed.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shows the regions of the image which gave the most activations for a certain label in a trained classification model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In simpler words, it tells us about the regions of the image which made the model decide that it belongs to a certain label &amp;ldquo;x&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/cell_detect_pipeline.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;And when the heatmap is superimposed upon the real image, it gives us an insight on how the CNN &amp;ldquo;looked&amp;rdquo; at the image&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Mind the bend</title>
      <link>https://mayukhdeb.github.io/blog/post/mind_the_bend/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/mind_the_bend/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/example.gif&#34; alt=&#34;example&#34;&gt;
&lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;The whole point of this project was to make a car stay on the road all by itself and defeat other players in a racing game called speed dreams.&lt;/p&gt;
&lt;h2 id=&#34;collection-of-training-data&#34;&gt;Collection of training data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rapidly takes screenshots of the game and saves them in a folder&lt;/li&gt;
&lt;li&gt;The label in this case is the x-co-ordinate of the mouse which is captured by pyautogui and is stored in the formatted filename of each collected image. Image fileame formatting is done as  &lt;code&gt;(x-value)_(unique_ID).png&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;processing-images&#34;&gt;Processing images&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Converts all images to numpy arrays with a depth of 3 for R,G and B color channels&lt;/li&gt;
&lt;li&gt;The shape gets changed from  &lt;code&gt;[ width, height, depth ]&lt;/code&gt; to &lt;code&gt;[ depth, width, height]&lt;/code&gt; for it to be of the right size for the CNN input channel&lt;/li&gt;
&lt;li&gt;Pairs them with the corresponding mouse x-coordinate as array &lt;code&gt;[  [[[image]]]  ,  [x-value]   ]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;all of the generated pairs are then stacked into one numpy array with the help of np.vstack() and saved&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-preprocessing-and-augmentation&#34;&gt;Data preprocessing and augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First things first, plotted the frequency distribution of each steering value hen&lt;/li&gt;
&lt;li&gt;Roughly doubled the amount of training data by generating mirror images of existing images and vstacking them with reversed steer value.&lt;/li&gt;
&lt;li&gt;Flattened the frequency distribution by making copies of random ramples of under-represented steering value data points&lt;/li&gt;
&lt;li&gt;Normalised the steering values by replacing the x-co-ordinates with steering values. In my case the &amp;ldquo;straight&amp;rdquo; steer value was at &lt;code&gt;x = 400&lt;/code&gt;, for &lt;code&gt;normalised_value = 400 - x_value&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;note :  Right is negative steer value and left is positive&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;self-driving&#34;&gt;Self driving&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rapidly taken screenshots are prerpocessed and fed to the trained CNN drunk_driver()&lt;/li&gt;
&lt;li&gt;&lt;code&gt;drunk_driver()&lt;/code&gt; returns a steer value&lt;/li&gt;
&lt;li&gt;Returned value is taken care  of by pyauogui which moves the mouse accordingly&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
