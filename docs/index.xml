<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>@mayukhdeb on @mayukhdeb </title>
      <generator uri="https://gohugo.io">Hugo</generator>
    <link>https://mayukhdeb.github.io/blog/</link>
    <language>en-us</language>
    
    
    <updated>Sun, 17 May 2020 00:00:00 UTC</updated>
    
    <item>
      <title>Hello Openworm</title>
      <link>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-17/</link>
      <pubDate>Sun, 17 May 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/gsoc-2020-may-17/</guid>
      <description>&lt;p&gt;Ten weeks of proposal writing and 3 personal (but very beloved) deep learning projects later, I was selected for the only project I worked towards for GSoC, which was &lt;a href=&#34;https://summerofcode.withgoogle.com/projects/#6078813261266944&#34;&gt;Pre-trained models for developmental neuroscience&lt;/a&gt; under &lt;a href=&#34;https://www.incf.org/&#34;&gt;INCF&lt;/a&gt;. And things have been better than ever since then.&lt;/p&gt;
&lt;p&gt;The first week was mostly me getting to know more about the OpenWorm community and what are they striving to do. I got to meet &lt;a href=&#34;https://github.com/balicea&#34;&gt;Dr. Bradley Alicea&lt;/a&gt;, &lt;a href=&#34;https://github.com/nvinayvarma189&#34;&gt;Vinay Varma&lt;/a&gt; and &lt;a href=&#34;https://github.com/ujjwalll&#34;&gt;Ujjwal Singh&lt;/a&gt;. They gave me a warm welcome into the community.&lt;/p&gt;
&lt;p&gt;I started setting up the environment right after the results were out, and cleaned up one of my &lt;a href=&#34;https://github.com/Mayukhdeb/adventures-with-augmentation&#34;&gt;older repos&lt;/a&gt; which would act as a quick reference through the span of this project.&lt;/p&gt;
&lt;p&gt;The first weekly meeting with OpenWorm gave me a much better idea of the goals of this project.&lt;/p&gt;
&lt;p&gt;The key to a good deep learning model is good data. If the data is clean, half the battle is won. This led to the the first &lt;a href=&#34;https://nbviewer.jupyter.org/github/devoworm/GSoC-2020/blob/master/Pre-trained%20Models%20%28DevLearning%29/notebooks/worm_tracking_pipeline.ipynb&#34;&gt;notebook&lt;/a&gt; which builds a pipeline for tracking and segmenting the worms directly from video files. The pipeline works as follows:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/may_17_visualization.png&#34; height=&#34;900&#34;/&gt; 
&lt;/figure&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The video file is first trimmed to shorten the clip to extract only a short subclip (this is optional).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All the frames from the subclip are extracted from the video using &lt;a href=&#34;https://zulko.github.io/moviepy/ref/ffmpeg.html&#34;&gt;ffmpeg&lt;/a&gt; and saved into a folder in a numerical sequence of filenames.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The locations of the worms are determined by drawing padded rectangles around the contours of the thresholded image.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/rectangles.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;The original image is then cropped and segmented to remove all background except the worm(s) in the image.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/segmented.png&#34; width=&#34;60%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Here&amp;rsquo;s a GIF to visualise the tracking on the live feed.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/may_17/tracking.gif&#34; width=&#34;80%&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Apart from all of this, I also read quite a few papers on the C.elegans worm. This was an effort towards getting more familiar with the underlying biological concepts surrounding the worm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep chicken saviour</title>
      <link>https://mayukhdeb.github.io/blog/post/deep-chicken-saviour/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/deep-chicken-saviour/</guid>
      <description>&lt;h2 id=&#34;before&#34;&gt;Before:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/no_FGSM.gif&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;after&#34;&gt;After:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/FGSM.gif&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;adversarial-blocks-generated-by-negative-epsilon-fgsm-to-confuse-the-traned-deep-neural-network&#34;&gt;Adversarial blocks generated by negative epsilon FGSM to confuse the traned deep neural network&lt;/h2&gt;
&lt;p&gt;These blocks are made by starting out with a dark grey image and then backpropagating on the image with the pre trained network with a negative epsilon in order to minimise loss for the target class, a more negative epsilon will not necessarily give a better result. But it&amp;rsquo;s a bell curve instead, and the epsilon is optimized by looking for the local target class probability maxima in the domain [&lt;code&gt;lower_limit&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;These adversarial blocks can be generated for any animal class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/patch_to_block.png&#34; alt=&#34;patch_to_block&#34;&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/adv_block.gif&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;fast-gradient-sign-method-chart_with_upwards_trend&#34;&gt;Fast Gradient Sign Method :chart_with_upwards_trend:&lt;/h2&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sign(data_gradients)&lt;/code&gt; gives the element wise signs of the data gradient&lt;/li&gt;
&lt;li&gt;&lt;code&gt;epsilon&lt;/code&gt; defines the &amp;ldquo;strength&amp;rdquo; of the perturbation of the image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In a nutshell, instead of &lt;strong&gt;optimizing the model to reduce the loss&lt;/strong&gt;, we&amp;rsquo;re &lt;strong&gt;un-optimizing the input image to maximise loss&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This works primarily because of the piecewise linear nature of deep neural networks. For example, look at ReLU or at maxout functions, they&amp;rsquo;re all piecewise linear. Even a carefully tuned sigmoid has an approximate linear nature when taken piecewise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With varying values of epsilon, we will see an approximately linear relationship between &amp;ldquo;confidence&amp;rdquo; and epsilon.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;negative-epsilon-fgsm&#34;&gt;Negative epsilon FGSM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;this can be used to turn one animal into another specific animal for the deep neural network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/negative_eps_FGSM.png&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-are-the-images-optimized-using-the-same-fgsm-&#34;&gt;How are the images &amp;ldquo;optimized&amp;rdquo; using the same FGSM ?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The key here is to understand how FGSM actually worked.
In FGSM, we were tampering with the pixels which has a &lt;em&gt;positive&lt;/em&gt; gradient and added a certain value &lt;code&gt;gradient * epsilon&lt;/code&gt; to each of those pixels. This made the image deviate further and further away from the class it actually belongs to and thus maximising loss in the process. Note that this was done with a &lt;strong&gt;positive epsilon&lt;/strong&gt; value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But for our current objective, we will try to &amp;ldquo;optimize&amp;rdquo; the image to a different class. This can be done by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Doing a forward pass with an image of class &lt;code&gt;x&lt;/code&gt; and with a label of &lt;code&gt;y&lt;/code&gt;. Where &lt;code&gt;y&lt;/code&gt; is the class to which we want to convert our image to.&lt;/li&gt;
&lt;li&gt;Performing a backpropagation on the network and extracting the gradients on the input image.&lt;/li&gt;
&lt;li&gt;Now instead of trying to maximise loss using the FGSM, we&amp;rsquo;ll reduce the loss with a &lt;strong&gt;negative epsilon&lt;/strong&gt; FGSM.&lt;/li&gt;
&lt;li&gt;This will help reduce the loss of the image with respect to the target class &lt;code&gt;y&lt;/code&gt;, and with a sufficiently  negative epsilon value, the image gets mis-classified as the target class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you didn&amp;rsquo;t read the boring stuff above, just remember that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;positive epsilon&lt;/strong&gt; value will &lt;strong&gt;un-optimize&lt;/strong&gt; the image&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;negative epsilon&lt;/strong&gt; value will &lt;strong&gt;optimize&lt;/strong&gt; the image for the given label class&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generating-adversarial-patches-from-black-images&#34;&gt;Generating adversarial patches from black images&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-saviour/master/images/adversarial_patches.png&#34; alt=&#34;no_FGSM&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep chicken terminator</title>
      <link>https://mayukhdeb.github.io/blog/post/deep-chicken-terminator/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/deep-chicken-terminator/</guid>
      <description>&lt;h1 id=&#34;deep-chicken-terminator&#34;&gt;deep-chicken-terminator&lt;/h1&gt;
&lt;p&gt;deep learning to track (and possibly kill)  chicken in minecraft&lt;/p&gt;
&lt;h2 id=&#34;step-1---collecting-training-data-for-the-deep-neural-network&#34;&gt;Step 1 - collecting training data for the deep neural network&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/training_data_samples.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was mostly just me taking cropped screenshots of animals while playing the game, took about 40 screenshots of each animal.&lt;/li&gt;
&lt;li&gt;The 20 screenshots of each animal were then augmented and got 500 samples of each.&lt;/li&gt;
&lt;li&gt;The dataset is very small, but it works anyways for now&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-2---training-a-deep-learning-model-on-the-samples&#34;&gt;Step 2 - training a deep learning model on the samples&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/training_curve.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The architecture was kept intentionally small so that it keeps a good response time on the live feed&lt;/li&gt;
&lt;li&gt;The dataset had only 2000 images sized at 50*50, so training barely took any time&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;step-3---detecting-a-chicken-or-any-animal-for-that-matter-from-an-image&#34;&gt;Step 3 - detecting a chicken (or any animal for that matter) from an image&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/pre_barbecue.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was done using a custom &lt;code&gt;detect_animal_numpy()&lt;/code&gt; function which iterates through the image with a certain kernel size and a stride size, and feeds each sample to the trained NN (nicknamed hunter)&lt;/li&gt;
&lt;li&gt;A heatmap is then generated from the output of the NN which gives us a probability distribution over the image of a certain animal ( chicken, pig, or panda)&lt;/li&gt;
&lt;li&gt;why use heatmaps instead of rectangles ? because they look cooler.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/deep-chicken-terminator/master/sample_images/barbecue.png&#34; alt=&#34;dataset&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adventures with augmentation</title>
      <link>https://mayukhdeb.github.io/blog/post/adventures-with-augmentation/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/adventures-with-augmentation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/Mayukhdeb/adventures-with-augmentation/tree/master/&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/bfeb5472ee3df9b7c63ea3b260dc0c679be90b97/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656e6465722d6e627669657765722d6f72616e67652e7376673f636f6c6f72423d66333736323626636f6c6f72413d346434643464&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;parallel-cnns-work-just-as-good-as-they-look&#34;&gt;Parallel CNNs work just as good as they look&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/parallel_cnn.jpg&#34; alt=&#34;parallel_cnns&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;but-why-use-them-anyways-&#34;&gt;But why use them anyways ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Because when two different architectures are trained on the same training set, they don&amp;rsquo;t have the same weaknesses (i.e different confusion matrices)&lt;/li&gt;
&lt;li&gt;This means that when both are combined, they tend to neutralise each other&amp;rsquo;s weaknesses, which gives us a boost in accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;class-activation-heatmaps-on-deep-neural-networks&#34;&gt;Class activation heatmaps on deep neural networks&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/original_comma.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/heatmap.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/superimposed.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shows the regions of the image which gave the most activations for a certain label in a trained classification model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In simpler words, it tells us about the regions of the image which made the model decide that it belongs to a certain label &amp;ldquo;x&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Mayukhdeb/adventures-with-augmentation/master/sample_images/cell_detect_pipeline.png&#34; alt=&#34;comma&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;And when the heatmap is superimposed upon the real image, it gives us an insight on how the CNN &amp;ldquo;looked&amp;rdquo; at the image&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Mind the bend</title>
      <link>https://mayukhdeb.github.io/blog/post/mind_the_bend/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 UTC</pubDate>
      
      <guid>https://mayukhdeb.github.io/blog/post/mind_the_bend/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/example.gif&#34; alt=&#34;example&#34;&gt;
&lt;img src=&#34;https://mayukhdeb.github.io/blog/post/images/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;The whole point of this project was to make a car stay on the road all by itself and defeat other players in a racing game called speed dreams.&lt;/p&gt;
&lt;h2 id=&#34;collection-of-training-data&#34;&gt;Collection of training data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rapidly takes screenshots of the game and saves them in a folder&lt;/li&gt;
&lt;li&gt;The label in this case is the x-co-ordinate of the mouse which is captured by pyautogui and is stored in the formatted filename of each collected image. Image fileame formatting is done as  &lt;code&gt;(x-value)_(unique_ID).png&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;processing-images&#34;&gt;Processing images&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Converts all images to numpy arrays with a depth of 3 for R,G and B color channels&lt;/li&gt;
&lt;li&gt;The shape gets changed from  &lt;code&gt;[ width, height, depth ]&lt;/code&gt; to &lt;code&gt;[ depth, width, height]&lt;/code&gt; for it to be of the right size for the CNN input channel&lt;/li&gt;
&lt;li&gt;Pairs them with the corresponding mouse x-coordinate as array &lt;code&gt;[  [[[image]]]  ,  [x-value]   ]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;all of the generated pairs are then stacked into one numpy array with the help of np.vstack() and saved&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-preprocessing-and-augmentation&#34;&gt;Data preprocessing and augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First things first, plotted the frequency distribution of each steering value hen&lt;/li&gt;
&lt;li&gt;Roughly doubled the amount of training data by generating mirror images of existing images and vstacking them with reversed steer value.&lt;/li&gt;
&lt;li&gt;Flattened the frequency distribution by making copies of random ramples of under-represented steering value data points&lt;/li&gt;
&lt;li&gt;Normalised the steering values by replacing the x-co-ordinates with steering values. In my case the &amp;ldquo;straight&amp;rdquo; steer value was at &lt;code&gt;x = 400&lt;/code&gt;, for &lt;code&gt;normalised_value = 400 - x_value&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;note :  Right is negative steer value and left is positive&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;self-driving&#34;&gt;Self driving&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rapidly taken screenshots are prerpocessed and fed to the trained CNN drunk_driver()&lt;/li&gt;
&lt;li&gt;&lt;code&gt;drunk_driver()&lt;/code&gt; returns a steer value&lt;/li&gt;
&lt;li&gt;Returned value is taken care  of by pyauogui which moves the mouse accordingly&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
